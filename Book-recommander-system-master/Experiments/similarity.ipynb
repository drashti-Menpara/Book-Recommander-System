{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  Trupti is a good girl.\n",
      "Sentence 2:  Trupti is a bad girl.\n",
      "Similarity index value :  0.9\n",
      "Similar\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords,wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import product\n",
    "import numpy\n",
    "\n",
    "str1 = \"Trupti is a good girl.\"\n",
    "str2 = \"Trupti is a bad girl.\"\n",
    "# str1 = \"Cat is drinking water.\"\n",
    "# str2 = \"Lions eat flesh.\"\n",
    "# str1 = \"He loves to play football.\"\n",
    "# str2 = \"Football is his favourite sport.\"\n",
    "\n",
    "#str1 = \"I was given a card by her in the garden.\"\n",
    "#str2 = \"In the garden, she gave me a card.\"\n",
    "\n",
    "# str1 = \"Ballmer has been vocal in the past warning that Linux is a threat to Microsoft.\"\n",
    "# str2 = \"In the memo, Ballmer reiterated the open-source threat to Microsoft.\"\n",
    "# str1 = \"The boy is fetching water from the well.\"\n",
    "# str2 = \"The lion is running in the forest.\"\n",
    "# str1 = \"A school is a place where kids go to study.\"\n",
    "# str2 = \"School is an institution for children who want to study.\"\n",
    "# str1 = \"The world knows it has lost a heroic champion of justice and freedom.\"\n",
    "# str2 = \"The earth recognizes the loss of a valiant champion of independence and justice.\"\n",
    "# str1 = \"A cemetery is a place where dead people's bodies or their ashes are buried.\"\n",
    "# str2 = \"A graveyard is an area of land ,sometimes near a church, where dead people are buried.\" \n",
    "\n",
    "##---------------Defining stopwords for English Language---------------##\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "##---------------Initialising Lists---------------##\n",
    "filtered_sentence1 = []\n",
    "filtered_sentence2 = []\n",
    "lemm_sentence1 = []\n",
    "lemm_sentence2 = []\n",
    "sims = []\n",
    "temp1 = []\n",
    "temp2 = []\n",
    "simi = []\n",
    "final = []\n",
    "same_sent1 = []\n",
    "same_sent2 = []\n",
    "#ps = PorterStemmer()\n",
    "\n",
    "##---------------Defining WordNet Lematizer for English Language---------------##\n",
    "lemmatizer  =  WordNetLemmatizer()\n",
    "\n",
    "#myfile =  open('Text1.txt', 'r') \n",
    "#data=myfile.read().replace('\\n', '')\n",
    "##print(sent_tokenize(example_text))\n",
    "##\n",
    "##print(word_tokenize(example_text))\n",
    "\n",
    "##---------------Tokenizing and removing the Stopwords---------------##\n",
    "\n",
    "for words1 in word_tokenize(str1):\n",
    "    if words1 not in stop_words:\n",
    "        if words1.isalnum():\n",
    "            filtered_sentence1.append(words1)\n",
    "\n",
    "##---------------Lemmatizing: Root Words---------------##\n",
    "\n",
    "for i in filtered_sentence1:\n",
    "    lemm_sentence1.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "#print(lemm_sentence1)\n",
    "\n",
    "\n",
    "##---------------Tokenizing and removing the Stopwords---------------##\n",
    "\n",
    "for words2 in word_tokenize(str2):\n",
    "    if words2 not in stop_words:\n",
    "        if words2.isalnum():\n",
    "            filtered_sentence2.append(words2)\n",
    "\n",
    "##---------------Lemmatizing: Root Words---------------##\n",
    "\n",
    "for i in filtered_sentence2:\n",
    "    lemm_sentence2.append(lemmatizer.lemmatize(i))\n",
    "    \n",
    "#print(lemm_sentence2)\n",
    "\n",
    "##---------------Removing the same words from the tokens----------------##\n",
    "##for word1 in lemm_sentence1:\n",
    "##    for word2 in lemm_sentence2:\n",
    "##        if word1 == word2:\n",
    "##            same_sent1.append(word1)\n",
    "##            same_sent2.append(word2)\n",
    "##            \n",
    "##if same_sent1 != []:\n",
    "##   for word1 in same_sent1:\n",
    "##    lemm_sentence1.remove(word1)\n",
    "##if same_sent2 != []:\n",
    "##   for word2 in same_sent2:\n",
    "##    lemm_sentence2.remove(word2)\n",
    "##            \n",
    "##print(lemm_sentence1)\n",
    "##print(lemm_sentence2)\n",
    "\n",
    "##---------------Similarity index calculation for each word---------------##\n",
    "for word1 in lemm_sentence1:\n",
    "    simi =[]\n",
    "    for word2 in lemm_sentence2:\n",
    "        sims = []\n",
    "       # print(word1)\n",
    "        #print(word2)\n",
    "        syns1 = wordnet.synsets(word1)\n",
    "        #print(syns1)\n",
    "        #print(wordFromList1[0])\n",
    "        syns2 = wordnet.synsets(word2)\n",
    "        #print(wordFromList2[0])\n",
    "        for sense1, sense2 in product(syns1, syns2):\n",
    "            d = wordnet.wup_similarity(sense1, sense2)\n",
    "            if d != None:\n",
    "                sims.append(d)\n",
    "    \n",
    "        #print(sims)\n",
    "        #print(max(sims))\n",
    "        if sims != []:        \n",
    "           max_sim = max(sims)\n",
    "           #print(max_sim)\n",
    "           simi.append(max_sim)\n",
    "             \n",
    "    if simi != []:\n",
    "        max_final = max(simi)\n",
    "        final.append(max_final)\n",
    "\n",
    "#print(final)\n",
    "\n",
    "#        if max_sim >= 0.7:\n",
    "#           print(word1)\n",
    "#           print(word2)\n",
    "#           print('\\n')\n",
    "           \n",
    "#           if word1 not in temp1:\n",
    "#              temp1.append(word1)\n",
    "#           if word2 not in temp2:\n",
    "#              temp2.append(word2)   \n",
    "           #lemm_sentence1.remove(word1)\n",
    "           #lemm_sentence2.remove(word2)          \n",
    "        #if wordFromList1 and wordFromList2: \n",
    "          #  s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "           # list.append(s)\n",
    "#for word1 in temp1:\n",
    "#    lemm_sentence1.remove(word1)\n",
    "\n",
    "#for word2 in temp2:\n",
    "#    lemm_sentence2.remove(word2)\n",
    "    \n",
    "#print(lemm_sentence1)\n",
    "#print(lemm_sentence2)\n",
    "\n",
    "\n",
    "##---------------Final Output---------------##\n",
    "\n",
    "similarity_index = numpy.mean(final)\n",
    "similarity_index = round(similarity_index , 2)\n",
    "print(\"Sentence 1: \",str1)\n",
    "print(\"Sentence 2: \",str2)\n",
    "print(\"Similarity index value : \", similarity_index)\n",
    "\n",
    "if similarity_index>0.8:\n",
    "    print(\"Similar\")\n",
    "elif similarity_index>=0.6:\n",
    "    print(\"Somewhat Similar\")\n",
    "else:\n",
    "    print(\"Not Similar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
